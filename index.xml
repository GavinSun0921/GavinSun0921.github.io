<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home</title>
    <link>https://gavinsun0921.github.io/</link>
    <description>Recent content on Home</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>&amp;copy; 2023 &lt;a href=&#34;https://github.com/GavinSun0921&#34;&gt;Gavin Sun&lt;/a&gt;
</copyright>
    <lastBuildDate>Fri, 12 May 2023 12:34:01 +0800</lastBuildDate><atom:link href="https://gavinsun0921.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Difference between &#39;Introduction&#39; and &#39;Related Work&#39;</title>
      <link>https://gavinsun0921.github.io/posts/difference-between-introduction-and-related-work/</link>
      <pubDate>Fri, 12 May 2023 12:34:01 +0800</pubDate>
      
      <guid>https://gavinsun0921.github.io/posts/difference-between-introduction-and-related-work/</guid>
      <description>Description In the writing of scientific papers, there are basically two chapters: Introduction and Related Work. This is a common issue, that many people get confused how to organize introduction and related work chapters. Because both are related to the literature review.
References [1] Bob, “Difference between introduction and related work?,” Academia Stack Exchange, Oct. 31, 2016. https://academia.stackexchange.com/q/79164 (accessed May 12, 2023).</description>
    </item>
    
    <item>
      <title>Personal Pipeline for Deep Learning</title>
      <link>https://gavinsun0921.github.io/posts/personal-pipeline/</link>
      <pubDate>Thu, 11 May 2023 18:11:41 +0800</pubDate>
      
      <guid>https://gavinsun0921.github.io/posts/personal-pipeline/</guid>
      <description>Weights &amp;amp; Biases: Weights &amp;amp; Biases makes it easy to track your experiments, manage &amp;amp; version your data, and collaborate with your team so you can focus on building the best models.
Accelerate: Accelerate is a library that enables the same PyTorch code to be run across any distributed configuration by adding just four lines of code! In short, training and inference at scale made simple, efficient and adaptable.</description>
    </item>
    
  </channel>
</rss>
